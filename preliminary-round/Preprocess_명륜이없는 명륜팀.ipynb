{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/janechoi/Desktop/CONTEST/DATACAMP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#basic libraries \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_palette(\"colorblind\")\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train = pd.read_csv('sentiment_dtm_train.csv',encoding='CP949')\n",
    "\n",
    "# def load_dataset(traindir, testdir):\n",
    "#     train = pd.read_csv(traindir,encoding='CP949')\n",
    "#     test = pd.read_csv(testdir,encoding='CP949')\n",
    "    \n",
    "#     #ID not needed\n",
    "# #     train.drop('Id', axis =1 , inplace=True )\n",
    "# #     test.drop('Id', axis= 1, inplace=True)\n",
    "    \n",
    "#     return train, test \n",
    "\n",
    "\n",
    "# train, test =load_dataset('sentiment_dtm_train.csv', 'sentiment_dtm_test.csv')\n",
    "# test.head()\n",
    "\n",
    "# # train.sum(axis = 1)\n",
    "\n",
    "# # train.columns #9452 rows × 302 columns (id, label)\n",
    "\n",
    "# train.shape\n",
    "# train\n",
    "# train['label'].value_counts() #5402, 4050\n",
    "# train\n",
    "\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# avoid_cols = ['label', 'Id']\n",
    "\n",
    "# cols = [col for col in train.columns if col not in avoid_cols]\n",
    "# len(cols) #300\n",
    "# skf = list(StratifiedKFold(n_splits=5,random_state=1008).split(train, train['label'])) \n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(train[cols], \n",
    "\n",
    "#                                                     train['label'] , \n",
    "\n",
    "#                                                     test_size=0.3, \n",
    "\n",
    "#                                                     stratify=train['label'], \n",
    "\n",
    "#                                                     random_state=1004)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL LOADING CODE \n",
    "\n",
    "\n",
    "def load_dataset(traindir, testdir):\n",
    "    train = pd.read_csv(traindir,encoding='CP949')\n",
    "    test = pd.read_csv(testdir,encoding='CP949')\n",
    "    \n",
    "    #ID not needed\n",
    "#     train.drop('Id', axis =1 , inplace=True)\n",
    "#     test.drop('Id', axis= 1, inplace=True)\n",
    "\n",
    "    import copy\n",
    "    train['w_cnt'] = train[train.columns.difference(['Id'])].sum(axis = 1)\n",
    "    train['w_perc'] = train['w_cnt'].copy()/300\n",
    "    test['w_cnt'] = test[test.columns.difference(['Id'])].sum(axis = 1)\n",
    "    test['w_perc'] = test['w_cnt'].copy()/300\n",
    "    \n",
    "    return train, test \n",
    "\n",
    "train, test =load_dataset('sentiment_dtm_train.csv', 'sentiment_dtm_test.csv')\n",
    "\n",
    "train.shape, test.shape #((9452, 302), (9000, 301))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "avoid_cols = ['label', 'Id']\n",
    "\n",
    "cols = [col for col in train.columns if col not in avoid_cols]\n",
    "len(cols) #300\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train[cols], \n",
    "\n",
    "                                                    train['label'] , \n",
    "\n",
    "                                                    test_size=0.3, \n",
    "\n",
    "                                                    stratify=train['label'], \n",
    "\n",
    "                                                    random_state=1004)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: 0.861597, 0.0056\n",
      "SVC: 0.962360, 0.0039\n",
      "DT: 0.895958, 0.0074\n",
      "RF: 0.925768, 0.0045\n",
      "ADA: 0.895029, 0.0049\n",
      "GB: 0.898286, 0.0014\n",
      "Gaussian: 0.879298, 0.0020\n",
      "LR: 0.983678, 0.0021\n"
     ]
    }
   ],
   "source": [
    "#simple train model \n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix \n",
    "import lightgbm as lgb \n",
    "import catboost \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "models = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "\tAdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    GaussianNB(),\n",
    "    LogisticRegression()]\n",
    "    \n",
    "\n",
    "names = [\"KNN\", \"SVC\", \"DT\", \"RF\", \"ADA\", \"GB\", \"Gaussian\", \"LR\",]\n",
    "\n",
    "\n",
    "# define cross validation strategy\n",
    "\n",
    "def accu_cv(model,X,y):\n",
    "    acc = np.sqrt(cross_val_score(model, X, y, cv=skf, scoring = 'accuracy'))\n",
    "    return acc\n",
    "\n",
    "for name, model in zip(names, models):\n",
    "    score = accu_cv(model, train[cols], train['label'] )\n",
    "    print(\"{}: {:.6f}, {:.4f}\".format(name,score.mean(),score.std()))\n",
    "    \n",
    "\n",
    "# KNN: 0.861597, 0.0056\n",
    "\n",
    "# SVC: 0.962360, 0.0039\n",
    "\n",
    "# DT: 0.895958, 0.0074\n",
    "\n",
    "# RF: 0.925768, 0.0045\n",
    "\n",
    "# ADA: 0.895029, 0.0049\n",
    "# GB: 0.898286, 0.0014\n",
    "# Gaussian: 0.879298, 0.0020\n",
    "\n",
    "# LR: 0.983678, 0.0021\n",
    "  \n",
    "    \n",
    "###SVC , RF , LR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | learni... | max_depth | n_esti... | reg_alpha | reg_la... |\n",
      "-------------------------------------------------------------------------------------\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.8868  \u001b[0m | \u001b[0m 0.07293 \u001b[0m | \u001b[0m 15.73   \u001b[0m | \u001b[0m 801.4   \u001b[0m | \u001b[0m 0.6635  \u001b[0m | \u001b[0m 0.5847  \u001b[0m |\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.8878  \u001b[0m | \u001b[0m 0.07875 \u001b[0m | \u001b[0m 11.56   \u001b[0m | \u001b[0m 945.9   \u001b[0m | \u001b[0m 0.7891  \u001b[0m | \u001b[0m 0.5767  \u001b[0m |\n",
      "Fold: 0\n",
      "Fold: 1\n",
      "Fold: 2\n",
      "Fold: 3\n",
      "Fold: 4\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m-0.8602  \u001b[0m | \u001b[95m 0.08804 \u001b[0m | \u001b[95m 5.22    \u001b[0m | \u001b[95m 500.1   \u001b[0m | \u001b[95m 0.557   \u001b[0m | \u001b[95m 0.5321  \u001b[0m |\n",
      "Fold: 0\n",
      "Fold: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (0.08225558302425759, 19.73276484069888, 500.6610089859156, 0.6506692767536386, 0.5435758362409613)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-c22d6820c875>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m#randomly use 10 initial points, and use bayesian for 100 more rounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mxgbBO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0mxgbBO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZATION_END\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-c22d6820c875>\u001b[0m in \u001b[0;36mxgb_eval\u001b[0;34m(max_depth, learning_rate, n_estimators, reg_lambda, reg_alpha)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrn_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrn_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mother_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0meval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/xgboost-0.81-py3.7.egg/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    711\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/xgboost-0.81-py3.7.egg/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/xgboost-0.81-py3.7.egg/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/xgboost-0.81-py3.7.egg/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#xgboost -bayesian \n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "\n",
    "#BAYESIAN OPTIMIZATION \n",
    "\n",
    "\n",
    "def xgb_eval(max_depth,learning_rate, n_estimators, reg_lambda, reg_alpha ):\n",
    "    \n",
    "    \n",
    "    skf = list(StratifiedKFold(n_splits=5,random_state=1004).split(X_train, y_train) )\n",
    "    \n",
    "    train = X_train[cols]\n",
    "    val = X_val[cols]\n",
    "    \n",
    "    #select params to use \n",
    "    params = {'booster' : 'gbtree',\n",
    "            'objective' :'binary:logistic',\n",
    "             'eval_metric' : \"error\", # #(wrong cases)/#(all cases). \n",
    "            'random_state' : 314,  \n",
    "            'verbosity' :2, # 0 (silent), 1 (warning), 2 (info), 3 (debug)\n",
    "             'silent' : False,\n",
    "             'nthread':3 }\n",
    "    \n",
    "    #optimize parametes :: \n",
    "    params[\"max_depth\"] = int(round(max_depth))\n",
    "    params['learning_rate'] = learning_rate\n",
    "    params['n_estimators'] = int(round(n_estimators))\n",
    "    params['reg_lambda'] = max(reg_lambda, 0)\n",
    "    params['reg_alpha'] = max(reg_alpha, 0)\n",
    " \n",
    "\n",
    "    #use cv to get result \n",
    "    \n",
    "    \n",
    "    res =  xgb.XGBClassifier(**params)\n",
    "    best=[]\n",
    "    \n",
    "    \n",
    "    #use skf for 5 fold seperatly from train data (not validation data )\n",
    "    for fold_, (trn_, val_) in enumerate(skf):\n",
    "        print(\"Fold:\",fold_)\n",
    "    \n",
    "        train_x, train_y = train.iloc[trn_], y_train.iloc[trn_]\n",
    "        other_x, other_y = train.iloc[val_], y_train.iloc[val_]\n",
    "        res.fit(train_x, train_y ,eval_set = [(val, y_val)],verbose=False)\n",
    "    \n",
    "        pred= res.predict(other_x)\n",
    "    \n",
    "        error =accuracy_score(other_y, pred) \n",
    "        best.append(error)\n",
    "        \n",
    "#     print(best)\n",
    "    mean = np.mean(np.array(best))\n",
    "    \n",
    "#     print(mean)\n",
    "    return  -(mean) #maximize :: minimize minus mae \n",
    "\n",
    "\n",
    "xgbBO = BayesianOptimization(xgb_eval, {'max_depth':(5,20),\n",
    "                                        'n_estimators':(500,1000), \n",
    "                                        'learning_rate':(0.04,0.1), #default 0.3 \n",
    "                                        'reg_lambda':(0.5,0.7), \n",
    "                                        'reg_alpha' :(0.5,0.8)},\n",
    "                                        random_state= 0 )\n",
    "\n",
    "#randomly use 10 initial points, and use bayesian for 100 more rounds \n",
    "xgbBO.maximize(init_points=2, n_iter=5)  \n",
    "xgbBO.max \n",
    "\n",
    "\n",
    "\n",
    "#prediction for xgboost \n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score,StratifiedKFold\n",
    "\n",
    "# |   iter    |  target   | learni... | max_depth | n_esti... | reg_alpha | reg_la... |\n",
    "#  -0.8602   |  0.08804  |  5.22     |  500.1    |  0.557    |  0.5321   |\n",
    "avoid_cols = ['label', 'Id']\n",
    "\n",
    "cols = [col for col in train.columns if col not in avoid_cols]\n",
    "len(cols) #300\n",
    "\n",
    "\n",
    "\n",
    "def xgb_cv(c=None,v=None):\n",
    "    \n",
    "    \n",
    "    skf = list(StratifiedKFold(n_splits=5,random_state=1004).split(X_train, y_train) )\n",
    "    \n",
    "    train = X_train[cols]\n",
    "    val = X_val[cols]\n",
    "    \n",
    "    params={}\n",
    "    #select params to use \n",
    "    basicparams = {'booster' : 'gbtree',\n",
    "            'objective' :'binary:logistic',\n",
    "             'eval_metric' : \"error\", # #(wrong cases)/#(all cases). \n",
    "            'random_state' : 314,  \n",
    "            'verbosity' :2, # 0 (silent), 1 (warning), 2 (info), 3 (debug)\n",
    "             'silent' : False,\n",
    "             'nthread':3 }\n",
    "    \n",
    "    #optimize parametes :: \n",
    "    params[\"max_depth\"] = int(round(5.22 ))\n",
    "    params['learning_rate'] =  0.08804  \n",
    "    params['n_estimators'] = int(round( 500.1))\n",
    "    params['reg_lambda'] = max( 0.5321 , 0)\n",
    "    params['reg_alpha'] = max( 0.557 , 0)\n",
    " \n",
    "    params.update(basicparams)\n",
    "    \n",
    "    #fit on xgboost regressor \n",
    "    train_x =X_train[cols]\n",
    "    val = X_val[cols]\n",
    "    \n",
    "    res = xgb.XGBClassifier(**params)\n",
    "    res.fit(train_x, y_train ,eval_set = [(val, y_val)],verbose=False)\n",
    "    \n",
    "    #predictions \n",
    "    predictions = res.predict(X_val[cols])\n",
    "    error= accuracy_score(y_val, predictions)\n",
    "    \n",
    "    print('AC for validation set is...' + str(error))\n",
    "\n",
    "    #AC for validation set is...0.8515514809590973\n",
    "    return  res\n",
    "    \n",
    "\n",
    "res= xgb_cv()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CATBOOST \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import randint\n",
    "from random import uniform\n",
    "\n",
    "\n",
    "## split by stratifiedfold\n",
    "xy_train = pd.concat([X_train, y_train], axis=1)\n",
    "skf = list(StratifiedKFold(n_splits=5,random_state=1008).split(xy_train, xy_train['label'])) \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def catboost_pipline(X_train_data, X_test_data, y_train_data, y_test_data, \n",
    "                     model, param_dist, cv=5, scoring_fit = 'accuracy'):\n",
    "\n",
    "    n_iter_search = 10\n",
    "    rs = RandomizedSearchCV(estimator = model, \n",
    "                            param_distributions = param_dist,\n",
    "                            n_iter = n_iter_search,\n",
    "                            cv = skf,\n",
    "                            n_jobs = 1,\n",
    "                            scoring = scoring_fit,\n",
    "                            random_state = 314,\n",
    "                            verbose = 10,\n",
    "                            return_train_score = True\n",
    "                           )\n",
    "\n",
    "    fitted_model = rs.fit(X_train_data, y_train_data,\n",
    "                          verbose = False,\n",
    "                          eval_set = (X_test_data, y_test_data),\n",
    "                          early_stopping_rounds = 100,\n",
    "#                           metric = 'Accuracy'\n",
    "                         )\n",
    "    \n",
    "    # early stopping rounds, eval_set은 한쌍으로\n",
    "    # early stopping rounds의 기준은 eval set임\n",
    "    print(\"BEST PARAMS : \", rs.best_params_)\n",
    "    print(\"BEST SCORE : \", rs.best_score_)\n",
    "\n",
    "    pred = fitted_model.predict(X_test_data)\n",
    "    \n",
    "    return fitted_model, pred, rs\n",
    "\n",
    "\n",
    "avoid_cols = ['Id','label']\n",
    "cols = [col for col in X_train.columns if col not in avoid_cols] #store_id, 3,6,12month, month \n",
    "\n",
    "# cat = [X_train.columns.difference(['w_cnt', 'w_perc'])]\n",
    "cat = X_train.columns.difference(['w_cnt', 'w_perc']).to_numpy()\n",
    "\n",
    "param_dist = {'learning_rate' : np.linspace(0.05, 0.1), #uniform(0.05, 0.1)\n",
    "              'depth' : randint(5, 100),\n",
    "              'iterations' : randint(1000, 3000),\n",
    "              'l2_leaf_reg': np.linspace(0.2, 0.9),\n",
    "              'bagging_temperature' : randint(0, 1000),\n",
    "#               'border_count' : randint(5, 254)\n",
    "             }\n",
    "\n",
    "model = CatBoostClassifier(cat_features = cat,\n",
    "                          random_seed = 314,\n",
    "                          verbose = True,\n",
    "                          use_best_model = True,\n",
    "                          eval_metric = 'Accuracy',\n",
    "                          od_type = 'Iter',\n",
    "                          od_wait = 100,\n",
    "#                           bootstrap_type = 'Bayesian',\n",
    "#                           boosting_type = 'Ordered'\n",
    "                         )\n",
    "                          \n",
    "\n",
    "m, pred, rs = catboost_pipline(X_train[cols], X_val[cols], \n",
    "                               y_train, y_val, \n",
    "                               model, param_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9167842031029619\n",
      "0.9594499294781382\n",
      "0.8430888575458392\n"
     ]
    }
   ],
   "source": [
    "#SVC / LR /RF --using validation set \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svc = SVC(probability=True)\n",
    "svc.fit(X_train,y_train)\n",
    "svc_result =svc.predict(X_val)\n",
    "\n",
    "print(accuracy_score(y_val, svc_result))\n",
    "#0.9167842031029619\n",
    "\n",
    "\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "lr_result =lr.predict(X_val)\n",
    "\n",
    "print(accuracy_score(y_val, lr_result) )\n",
    "#0.9594499294781382\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "rf_result =rf.predict(X_val)\n",
    "\n",
    "print(accuracy_score(y_val, rf_result) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making submission file \n",
    "\n",
    "avoid_cols2 = [ 'Id']\n",
    "\n",
    "cols2 = [col for col in test.columns if col not in avoid_cols2]\n",
    "\n",
    "def submission(model, test):\n",
    "\n",
    "    #predictions \n",
    "    y_pred = model.predict(test[cols2])\n",
    "    submission_df = pd.DataFrame()\n",
    "    submission_df['Id'] = test['Id'] #ID\n",
    "    submission_df['label'] = y_pred\n",
    "    \n",
    "    return submission_df \n",
    "\n",
    "svc_test = submission(svc,test)\n",
    "lr_test = submission(lr,test)\n",
    "rf_test = submission(rf,test)\n",
    "xgb_test = submission(xgb,test)\n",
    "xgb_test.to_csv('rf_sub.csv', index=0)\n",
    "\n",
    "\n",
    "svc_test.to_csv('svc_sub.csv', index=0)\n",
    "lr_test.to_csv('lr_sub.csv', index=0)\n",
    "rf_test.to_csv('rf_sub.csv', index=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
